---
date: "2025-09-10"
title: "Ingegneria del dato: la spina dorsale invisibile del digitale"
image: "images/blog/ingegneria_dato.png"
author: "pietro-portolani"
draft: false
---

Dietro ogni applicazione digitale ci sono infrastrutture di dati che raccolgono, trasformano e rendono disponibili informazioni. Le architetture moderne, dai data lakehouse alle pipeline ELT, aprono nuove opportunitÃ  ma richiedono attenzione a costi e complessitÃ . ProporzionalitÃ , efficienza e sostenibilitÃ  diventano principi guida per sistemi davvero utili e durevoli.

## 1. Cos'Ã¨ lâ€™ingegneria del dato?

Quando si parla di **trasformazione digitale**, lâ€™attenzione tende a concentrarsi su tecnologie â€œspettacolariâ€: intelligenza artificiale, algoritmi predittivi, strumenti di automazione. Tuttavia, dietro queste innovazioni si cela una disciplina meno visibile ma assolutamente cruciale: **lâ€™ingegneria del dato** o *data engineering* in inglese.

Un buon paragone Ã¨ la rete elettrica o internet: nessuno le osserva direttamente, ma ci accorgiamo subito se smettono di funzionare. Allo stesso modo, senza ingegneria del dato non avremmo nÃ© analisi avanzate nÃ© sistemi di intelligenza artificiale basati sui dati.

Secondo [IBM](https://www.ibm.com/think/topics/data-engineering), lâ€™ingegneria del dato Ã¨ la pratica di progettare e costruire sistemi per lâ€™**aggregazione**, lo **stoccaggio** e lâ€™**analisi** di grandi quantitÃ  di dati. Questo lavoro Ã¨ alla base sia dei processi decisionali sia dellâ€™addestramento dei modelli di *machine learning*.

Le attivitÃ  principali includono:

* **Estrazione, trasformazione e caricamento (ETL)** dei dati;
* Creazione e manutenzione delle architetture di stoccaggio;
* Orchestrazione e gestione dei processi di flusso dati;
* Verifica di qualitÃ , sicurezza e tracciabilitÃ ;
* Fornitura di servizi di accesso e consumo dei dati.

---

## 2. Dal tradizionale al moderno

Per lungo tempo, il modello di riferimento Ã¨ stato lâ€™ETL, con architetture **centralizzate** come i **data warehouse**, dove i dati venivano caricati solo dopo essere stati puliti, aggregati, inseriti all'interno di una struttura ben definita e ottimizzati. Questa scelta garantiva potenza e velocitÃ , ma introduceva anche rigiditÃ  e inefficienze: duplicazioni di dati, calcoli ripetuti e inutili, overhead di gestione.

Negli ultimi anni, lo scenario Ã¨ cambiato. Oggi si preferisce il modello **ELT (Extract-Load-Transform)**: i dati vengono estratti e **salvati subito** nel contenitore finale, per poi essere trasformati solo quando servono.

Questa evoluzione Ã¨ direttamente collegata al passaggio dal **data warehouse** al **data lakehouse**:

* Il **data lake** consente di salvare qualsiasi tipo di dato (CSV, immagini, file di log, ecc.) senza schema predefinito. Offre **flessibilitÃ , costi contenuti e scalabilitÃ **, ma soffre di problemi di consistenza, gestione e performance.
* Il **data lakehouse** combina i vantaggi del data lake con quelli del data warehouse. Sopra lo strato di file introduce un **catalogo di metadati** che applica schemi, **transazioni ACID**, indicizzazione e controllo degli accessi. Inoltre, separa lâ€™archiviazione dallo strato computazionale, permettendo un aumento esponenziale di volume e potenza grazie ad architetture distribuite.

ğŸ“Œ **Nota**: Non mancano perÃ² gli **svantaggi**. La maggiore flessibilitÃ  dei lakehouse puÃ² portare a complessitÃ  di gestione piÃ¹ elevate, a costi imprevisti per orchestrare i vari livelli e alla necessitÃ  di competenze tecniche avanzate.

---

## 3. Il costo ambientale dei dati

Dietro ogni â€œcloudâ€, dove vengono ospitati e gestiti i dati, câ€™Ã¨ unâ€™infrastruttura fisica: i **data center**. Queste strutture sono diventate protagoniste anche per il loro **impatto ambientale**.

Il [*Environment, Social and Governance Report 2025*](https://www.infrastructuresummit.io/esg-report) di *Structure Research* evidenzia dati significativi:

* **Consumi energetici**: dallo 0.78% del consumo mondiale nel 2019 a circa lâ€™1.16% nel 2024 (310.6 TWh).
* **Emissioni di COâ‚‚**: da 42.3 milioni di tonnellate nel 2019 a 76.2 milioni nel 2024.
* **Efficienza**: migliorata, con un calo da 366.9 a 312.7 milioni di tonnellate di COâ‚‚ equivalente per GWh consumato.
* **Consumo idrico**: passato da 138.7 milioni di mÂ³ nel 2019 a 219.6 milioni nel 2024, usati per il raffreddamento delle rack.

Questa crescita dei consumi e delle emissioni ha stimolato la ricerca di contromisure: soluzioni emergenti come i sistemi di raffreddamento a circuito chiuso o il recupero del calore dei data center per il riscaldamento urbano stanno diventando parte integrante delle strategie di sostenibilitÃ  del settore.

â¡ï¸ In sintesi, i dati mostrano una crescita costante di consumi ed emissioni, mitigata solo parzialmente dai progressi in efficienza e sostenibilitÃ  dei data center.

---

## 4. Big Dataâ€¦ spesso non cosÃ¬ â€œbigâ€

Un altro mito da ridimensionare riguarda i **Big Data**.

Secondo unâ€™[analisi](https://www.fivetran.com/blog/how-do-people-use-snowflake-and-redshift) dei dati pubblicati da **Snowflake** e **Redshift**:

* Tra le query che coinvolgono almeno 1MB di dati, la **mediana** Ã¨ di 100MB.
* Solo il **99.9Â° percentile** raggiunge 300GB.
* La maggior parte delle query riguarda **inserimento o trasformazione** dei dati, cioÃ¨ attivitÃ  tipiche di ETL.

ğŸ‘‰ Questo significa che i â€œBig Dataâ€ riguardano una **minoranza di utenti**. Nella pratica quotidiana, la maggior parte delle organizzazioni lavora con volumi molto piÃ¹ contenuti. Non sorprende quindi che si parli sempre piÃ¹ di **Small Data**: dati piÃ¹ piccoli, ma piÃ¹ mirati e significativi.

---

## 5. Dove dovrebbe guardare lâ€™ingegneria del dato moderna

Dallâ€™analisi fatta emergono alcuni principi guida per il futuro dellâ€™ingegneria del dato:

* **ProporzionalitÃ **: i sistemi devono essere calibrati sulla reale dimensione del problema. Costruire architetture gigantesche per dati relativamente modesti significa sprecare risorse e denaro.
* **Efficienza ambientale**: progettare pipeline e algoritmi che ottimizzino lâ€™uso di energia e memoria (es. query ottimizzate, architetture serverless).
* **AgilitÃ **: sistemi in grado di ridimensionarsi dinamicamente in base al volume di dati da gestire.
* **Approccio artigianale**: soluzioni **su misura**, pensate ad hoc per il problema attuale, ma con uno sguardo al futuro, cosÃ¬ da non subire i cambiamenti ma anticiparli.

In definitiva, lâ€™ingegneria del dato non deve piÃ¹ essere vista solo come un insieme di strumenti tecnici, ma come una disciplina che unisce **precisione ingegneristica, sostenibilitÃ  ambientale ed efficienza economica**.

---

## ğŸ“Œ Cosa chiedere quando si sviluppano soluzioni di ingegneria del dato:

ğŸ” Abbiamo davvero bisogno di unâ€™infrastruttura â€œBig Dataâ€ o i nostri dati sono in realtÃ  â€œSmallâ€?

âš–ï¸ Lâ€™architettura scelta Ã¨ proporzionata al problema o rischiamo sovradimensionamento e costi superflui?

ğŸŒ± Sono state valutate le implicazioni ambientali (consumi energetici, uso di acqua, emissioni)?

âš¡ Le pipeline sono ottimizzate per ridurre sprechi di calcolo e memoria?

ğŸ› ï¸ La soluzione Ã¨ abbastanza agile e adattabile a futuri cambiamenti di scala?

ğŸ’° Abbiamo stimato i costi di overhead (es. servizi accesi h24 non necessari)?

> ğŸ‘‰ **Rispondere a queste domande significa non solo avere sistemi piÃ¹ sostenibili e performanti, ma anche contenere i costi e ridurre rischi strategici.**
